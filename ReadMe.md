# MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph

[![huggingface](https://img.shields.io/badge/huggingface-available-yellow)](https://huggingface.co/ICLRAnonymous/MedCEG) [![arXiv](https://img.shields.io/badge/arXiv-2512.XXXXX-b31b1b.svg)](#)

**MedCEG** is a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). 

## ğŸš€ Model Architecture & Pipeline

Our approach consists of two main stages:
1.  **Cold-Start**: We transform structured evidence graphs into natural language to teach the model logical dependencies.
2.  **Graph-guided Reinforcement Learning**: We use a Critical Evidence Graph (CEG) to provide dense, process-oriented rewards. 

![MedCEG Pipeline](images/pipeline.png)
*Figure: Overview of the MedCEG pipeline including Data Preparation and Graph-guided RL.* 

## ğŸ› ï¸ Training with VeRL

We utilize [**VeRL**](https://github.com/volcengine/verl) (Volcengine Reinforcement Learning) for the RLHF/RLAIF stage. Our specific **Process Reward** functions provided in the `code/reward` directory.

### 1. Start the Embedding Service

To ensure efficient calculation of semantic similarity during training without reloading the model repeatedly, we use a standalone FastAPI server for embeddings. This server manages a pool of `BGE-LARGE-EN-V1.5` models across available GPUs.

**Setup:** Update the `BGE_PATH` variable in `code/server/EmbeddingServer.py` to point to your local model path.

**Launch:**

```Bash
python code/server/EmbeddingServer.py
```

*Note: Ensure this server is accessible to your training nodes (default port 8000).*

### 2. Integration with VeRL

The files in `code/reward/` contain the custom reward functions tailored for MedCEG.

- **`GraphReward.py`**: Implements the core logic for calculating the composite reward: `Node Coverage`, `Structural Correctness`, and `Chain Completeness`.
- **`GraphMCQ.py`** & **`GraphOpenendQuestion.py`**: These scripts contain the `compute_score()` function, which acts as the primary interface for the reward model.

The reward function evaluates:

1. **Format Reward**: Checks for proper `<think>...</think>` tag usage.
2. **Accuracy Reward**: Checks the final answer against Ground Truth.
3. **Process Reward**: Extracts the graph from the `<think>` trace and compares it against the Ground Truth CEG using the Embedding Server.

## ğŸ’» Inference

The `Inference.py` script allows you to test the model. Our model weights are open-sourced at [HuggingFace](https://huggingface.co/LinjieMu/MedCEG).

```Python
import transformers
import torch

# 1. Load the model and tokenizer
model_id = "ICLRAnonymous/MedCEG"
tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

# 2. Define the question
question = "A 78-year-old Caucasian woman presented with..."
suffix = "\nPut your final answer in \\boxed{}."
messages = [{"role": "user", "content": question + suffix}]

# 3. Generate
input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
outputs = model.generate(input_ids, max_new_tokens=8196, do_sample=False)
decoded_response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
print(decoded_response)
```

## ğŸ“Š Data Description

`DataExample.jsonl` contains examples of our constructed data. Key fields include:

| **Key**                         | **Description**                                              |
| ------------------------------- | ------------------------------------------------------------ |
| `question`                      | The input clinical question.                                 |
| `answer`                        | The ground truth answer.                                     |
| `reasoning_content`             | The rewritten "thinking" process based on the CEG.           |
| `graph/triplets`                | The full Evidence Graph (EG).                                |
| `graph/core_reasoning_subgraph` | The **Critical Evidence Graph (CEG)** used for reward calculation. |

## ğŸ† Experimental Results

### Main Results

MedCEG achieves state-of-the-art performance across multiple medical benchmarks, showing significant improvements in both accuracy and reasoning quality.

![main_results](./images/main-results.png)

*Table 1: Comprehensive performance comparison across ID and OOD benchmarks.* 

### Reasoning Process Quality

We evaluated the reasoning process across five dimensions: Logical Coherence, Factual Accuracy, Evidence Faithfulness, Interpretability & Clarity, and Information Utilization. MedCEG significantly outperforms baselines in producing clinically sound reasoning.

<img src="./images/process.png" alt="process_results" style="zoom:50%; display: block; margin: 0 auto;" />



*Figure 2: Multi-dimensional evaluation of the reasoning process.* 

## ğŸ“ File Structure

```text
â”‚   ğŸ“„ ReadMe.md
â”‚â”€â”€â”€ğŸ“ code
â”‚   â””â”€â”€â”€ğŸ“„ DataExample.jsonl
â”œâ”€â”€â”€ğŸ“ code
â”‚   â”œâ”€â”€â”€ğŸ“ evaluation
â”‚   â”‚       ğŸ“„ ProcessEvaluation.py   # Evaluate reasoning process precision
â”‚   â”‚
â”‚   â”œâ”€â”€â”€ğŸ“ reward                     # Core Reward Logic for VeRL
â”‚   â”‚       ğŸ“„ graph_extract.py       # Extract triplets from reasoning text via LLM
â”‚   â”‚       ğŸ“„ GraphReward.py         # Calculate graph-based rewards (Node, Struct, Chain)
â”‚   â”‚       ğŸ“„ GraphMCQ.py            # Reward entry point for Multiple Choice Questions
â”‚   â”‚       ğŸ“„ GraphOpenendQuestion.py# Reward entry point for Open-ended Questions
â”‚   â”‚       ğŸ“„ TripletsRecall.py      # Utility for calculating triplet recall
â”‚   â”‚
â”‚   â””â”€â”€â”€ğŸ“ server
â”‚           ğŸ“„ EmbeddingServer.py     # FastAPI server for BGE embeddings
â”‚
â””â”€â”€â”€ğŸ“ results                        # Raw output files from benchmarks
        ğŸ“„ DiagArena.jsonl
        ğŸ“„ MedBullets-5op.jsonl
        ğŸ“„ MedCase.jsonl
        ğŸ“„ MedQA.jsonl
        ğŸ“„ MMLU-health.jsonl
        ğŸ“„ MMLU-Pro-Health.jsonl
```

## 